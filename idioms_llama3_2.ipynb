{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPv1rSjzbDhRyi6TTM4HGYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eddiechu888/idioms/blob/main/idioms_llama3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "1d_E_IitJHMc",
        "outputId": "e2f5e29d-093b-400d-e428-52b902d7108b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "Using device: cuda\n",
            "Hooks registered on model layers.\n",
            "\n",
            "Processing 'idiomatic' sentence:\n",
            "Sentence: She thought of good ways to break the ice at the company retreat.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9be12c127069>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Process the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Store activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9be12c127069>\u001b[0m in \u001b[0;36mprocess_sentence\u001b[0;34m(sentence, max_length)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# 4. Function to Process a Sentence and Capture Activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     encoded_input = tokenizer(\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3053\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3055\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3056\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3057\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3161\u001b[0m             )\n\u001b[1;32m   3162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3163\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   3164\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3228\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   3229\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2957\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2959\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2960\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from getpass import getpass\n",
        "\n",
        "# 1. Setup\n",
        "hf_token = getpass('Enter your Hugging Face token: ')\n",
        "\n",
        "if hf_token is None:\n",
        "    raise ValueError(\"HUGGINGFACE_TOKEN is not set.\")\n",
        "\n",
        "model_name = 'meta-llama/Llama-3.2-1B'\n",
        "\n",
        "# Check for CUDA and MPS availability and use the best available option\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize tokenizer and model with the HF token for authentication\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
        "\n",
        "# Set pad_token to eos_token if pad_token is not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"Pad token not found. Setting pad_token to eos_token: {tokenizer.pad_token}\")\n",
        "else:\n",
        "    print(f\"Using existing pad_token: {tokenizer.pad_token}\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hf_token).to(device)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# 2. Define the Single Data Example\n",
        "single_example = {\n",
        "    \"idiomatic\": \"She thought of good ways to break the ice at the company retreat.\",\n",
        "    \"non_idiomatic\": \"She thought of good ways to get people talking at the company retreat.\",\n",
        "    \"literal\": \"She thought of good ways to break the ice to make a cold drink.\"\n",
        "}\n",
        "\n",
        "# 3. Define Hook for Activations\n",
        "activations = defaultdict(list)\n",
        "\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        # If output is a tuple, extract the first element (hidden states)\n",
        "        if isinstance(output, tuple):\n",
        "            output = output[0]\n",
        "\n",
        "        # If output is a list, extract the first element\n",
        "        if isinstance(output, list):\n",
        "            output = output[0]\n",
        "\n",
        "        # Check if the output is a Tensor\n",
        "        if isinstance(output, torch.Tensor):\n",
        "            print(f\"Hook '{name}': Capturing Tensor with shape {output.shape}\")\n",
        "            activations[name].append(output.detach().cpu())\n",
        "        else:\n",
        "            print(f\"Hook '{name}': Output is of type {type(output)}, skipping.\")\n",
        "    return hook\n",
        "\n",
        "# Register hooks on desired layers\n",
        "# It's crucial to inspect the model architecture to correctly access the transformer layers.\n",
        "# For LLaMA models, the transformer layers are typically under model.model.layers\n",
        "# Adjust the attribute path if necessary.\n",
        "\n",
        "for i, layer in enumerate(model.model.layers):\n",
        "    layer_name = f'layer_{i}'\n",
        "    layer.register_forward_hook(get_activation(layer_name))\n",
        "\n",
        "print(\"Hooks registered on model layers.\")\n",
        "\n",
        "# 4. Function to Process a Sentence and Capture Activations\n",
        "def process_sentence(sentence, max_length=20):\n",
        "    encoded_input = tokenizer(\n",
        "        sentence,\n",
        "        return_tensors='pt',\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        model(**encoded_input)\n",
        "    # After forward pass, activations are populated via hooks\n",
        "    return\n",
        "\n",
        "# 5. Process Each Sentence in the Triplet\n",
        "conditions = ['idiomatic', 'non_idiomatic', 'literal']\n",
        "condition_activations = {condition: defaultdict(list) for condition in conditions}\n",
        "\n",
        "for condition in conditions:\n",
        "    sentence = single_example[condition]\n",
        "    print(f\"\\nProcessing '{condition}' sentence:\")\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "\n",
        "    # Clear previous activations\n",
        "    activations.clear()\n",
        "\n",
        "    # Process the sentence\n",
        "    process_sentence(sentence)\n",
        "\n",
        "    # Store activations\n",
        "    for layer, act in activations.items():\n",
        "        condition_activations[condition][layer].extend(act)\n",
        "\n",
        "    print(f\"Activations captured for '{condition}' condition.\")\n",
        "\n",
        "# 6. Analyze and Compare Activations\n",
        "# For simplicity, we'll compute the average activation for each layer and visualize differences.\n",
        "\n",
        "def compute_average_activation(activations_dict):\n",
        "    average_activation = {}\n",
        "    for layer, acts in activations_dict.items():\n",
        "        # Stack all activations for the layer (should be only one in this case)\n",
        "        stacked = torch.cat(acts, dim=0)  # Shape: [batch_size, ...]\n",
        "        # Compute the mean across the batch dimension\n",
        "        average = stacked.mean(dim=0)\n",
        "        average_activation[layer] = average\n",
        "    return average_activation\n",
        "\n",
        "avg_idiomatic = compute_average_activation(condition_activations['idiomatic'])\n",
        "avg_non_idiomatic = compute_average_activation(condition_activations['non_idiomatic'])\n",
        "avg_literal = compute_average_activation(condition_activations['literal'])\n",
        "\n",
        "# Example: Compare average activations of a specific layer\n",
        "layer_to_compare = 'layer_0'  # Change as needed\n",
        "\n",
        "idiom_avg = avg_idiomatic[layer_to_compare]\n",
        "non_idiom_avg = avg_non_idiomatic[layer_to_compare]\n",
        "literal_avg = avg_literal[layer_to_compare]\n",
        "\n",
        "# Compute differences\n",
        "difference_idiom_non_idiom = idiom_avg - non_idiom_avg\n",
        "difference_idiom_literal = idiom_avg - literal_avg\n",
        "\n",
        "print(f\"\\nAverage activation difference for {layer_to_compare} (Idiomatic - Non-Idiomatic): {difference_idiom_non_idiom}\")\n",
        "print(f\"Average activation difference for {layer_to_compare} (Idiomatic - Literal): {difference_idiom_literal}\")\n",
        "\n",
        "# 7. Visualize Activation Differences\n",
        "# We'll plot the activation differences as heatmaps.\n",
        "\n",
        "def plot_activation_difference(difference_tensor, layer_name, condition_pair):\n",
        "    difference_np = difference_tensor.numpy()\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(difference_np, cmap='coolwarm')\n",
        "    plt.title(f'Activation Differences in {layer_name} ({condition_pair})')\n",
        "    plt.xlabel('Neuron Index')\n",
        "    plt.ylabel('Hidden Units' if len(difference_np.shape) > 1 else 'Dimension')\n",
        "    plt.show()\n",
        "\n",
        "# Plotting the differences\n",
        "plot_activation_difference(difference_idiom_non_idiom, layer_to_compare, 'Idiomatic - Non-Idiomatic')\n",
        "plot_activation_difference(difference_idiom_literal, layer_to_compare, 'Idiomatic - Literal')"
      ]
    }
  ]
}